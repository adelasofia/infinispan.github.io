#!/usr/bin/env ruby

require 'date'
require 'fileutils'
require 'net/http'
require 'uri'
require 'rss'
require 'kramdown'
require 'nokogiri'

class Importer

  def initialize(import_file, output_dir)
    @import_file = import_file
    @output_dir  = output_dir
  end

  def import_file
    feed = RSS::Parser.parse( File.read( @import_file ) )
    feed.items.each do |item|
      if item.categories.find { |c| c.term == "http://schemas.google.com/blogger/2008/kind#post" }
        tags = item.categories.select { |c| c.scheme == "http://www.blogger.com/atom/ns#" }.map { |c| c.term }
        link = item.links.find { |l| l.rel == "alternate" }
        if link
          parts = link.href.split("/")
          title = item.title.content.gsub(':', '&#58;')
          title = title.gsub("'", '')
          puts "#{title}"
          linkified_title = parts[parts.length-1].gsub('.html' , '')
          published = DateTime.parse( item.published.content.to_s )
          pubDate = published.strftime("%A %d %B %Y")
          html = clean_html(item.content.content)
          #html = item.content.content
          content = Kramdown::Document.new(html, :html_to_native => true).to_kramdown
          output_path = File.join( @output_dir, published.strftime( "%Y-%m-%d-#{linkified_title}.md" ) )
          FileUtils.mkdir_p( File.dirname( output_path ) )
          File.open( output_path, 'w' ) do |f|
            f.puts "---"
            f.puts "layout: blog"
            f.puts "title: #{title}"
            f.puts "published: #{pubDate}"
            f.puts "author: #{item.author.name.content}"
            f.puts "tags: [ #{tags.join(", ")} ]"
            f.puts "---"
            f.puts ""
            f.puts content
          end
        end
      end
    end
  end

  private

  def clean_html(content)
    puts content
    puts "*************************************"
    doc = Nokogiri::XML::Document.parse( content )
    # Download images and link them locally
    doc.xpath( '//img' ).each do |img|
      src = img.attributes['src']
      
      puts "  Image #{src}"

      basename = File.basename( src )
      output_path = "#{@output_dir}/img/#{basename}"
 
      uri = URI.parse( src )
      begin
        res = Net::HTTP.get_response(uri)
        FileUtils.mkdir_p( File.dirname( output_path ) )
        File.open( output_path, 'wb' ) do |f|
          f.write res.body
        end

        img.attributes['src'] = "/#{output_path}"
      rescue SocketError
        puts "Could not fetch #{uri}"
      end
    end
    
    # Convert links to local and clean them
    doc.xpath('//a' ).each do |a|
      href = a.attributes['href'].to_s
      href = href.gsub("http://blog.infinispan.org/", "/#{@output_dir}/")
      
      puts "  Link #{href}"
      a.attributes['href'] = href
      a.remove_attribute('target')
    end
    
    # Remove divs but keep content
    doc.xpath('//div' ).each do |node|
    puts "################# REPLACING DIV"
      node.replace Nokogiri::XML::Text.new('<p>' + node.to_s + '</p>', node.document)
    end
    
    # Kramdown has trouble with th elements, convert them to td
    doc.xpath('//th' ).each do |node|
      puts "################# REPLACING TH"
      node.replace Nokogiri::XML::Text.new('<td>' + node.to_s + '</td>', node.document)
    end
    content = doc.to_html
    puts content
    puts "===================================="
    
    content
  end

end

if (ARGV.length != 2)
  puts "blogger_import xml target_directory"
else
  importer = Importer.new( ARGV[0], ARGV[1] )
  importer.import_file
end

